Project Introduction
Lightweight fine-tuning is one of the most important techniques for adapting foundation models, because it allows you to modify foundation models for your needs without needing substantial computational resources.

In this project, you will apply parameter-efficient fine-tuning using the Hugging Face peft library.

Project Summary
In this project, you will bring together all of the essential components of a PyTorch + Hugging Face training and inference process. Specifically, you will:

Load a pre-trained model and evaluate its performance
Perform parameter-efficient fine tuning using the pre-trained model
Perform inference using the fine-tuned model and compare its performance to the original model

Key Concepts
Hugging Face PEFT allows you to fine-tune a model without having to fine-tune all of its parameters.

Training a model using Hugging Face PEFT requires two additional steps beyond traditional fine-tuning:

Creating a PEFT config
Converting the model into a PEFT model using the PEFT config
Inference using a PEFT model is almost identical to inference using a non-PEFT model. The only difference is that it must be loaded as a PEFT model.

Training with PEFT
Creating a PEFT Config
The PEFT config specifies the adapter configuration for your parameter-efficient fine-tuning process. The base class for this is a PeftConfig, but this example will use a LoraConfig, the subclass used for low rank adaptation (LoRA).

A LoRA config can be instantiated like this:

from peft import LoraConfig
config = LoraConfig()
Look at the LoRA adapter documentation for additional hyperparameters that can be specified by passing arguments to LoraConfig(). Hugging Face LoRA conceptual guide(opens in a new tab) also contains additional explanations.

Converting a Transformers Model into a PEFT Model
Once you have a PEFT config object, you can load a Hugging Face transformers model as a PEFT model by first loading the pre-trained model as usual (here we load GPT-2):

from transformers import AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained("gpt2")
Then using get_peft_model() to get a trainable PEFT model (using the LoRA config instantiated previously):

from peft import get_peft_model
lora_model = get_peft_model(model, config)

Training with a PEFT Model
After calling get_peft_model(), you can then use the resulting lora_model in a training process of your choice (PyTorch training loop or Hugging Face Trainer).

Checking Trainable Parameters of a PEFT Model
A helpful way to check the number of trainable parameters with the current config is the print_trainable_parameters() method:

lora_model.print_trainable_parameters()
Which prints an output like this:

trainable params: 294,912 || all params: 124,734,720 || trainable%: 0.23643136409814364

Saving a Trained PEFT Model
Once a PEFT model has been trained, the standard Hugging Face save_pretrained() method can be used to save the weights locally. For example:

lora_model.save_pretrained("gpt-lora")
Note that this only saves the adapter weights and not the weights of the original Transformers model. Thus the size of the files created will be much smaller than you might expect.

Inference with PEFT
Loading a Saved PEFT Model
Because you have only saved the adapter weights and not the full model weights, you can't use from_pretrained() with the regular Transformers class (e.g., AutoModelForCausalLM). Instead, you need to use the PEFT version (e.g., AutoPeftModelForCausalLM). For example:

from peft import AutoPeftModelForCausalLM
lora_model = AutoPeftModelForCausalLM.from_pretrained("gpt-lora")
After completing this step, you can proceed to use the model for inference.

Generating Text from a PEFT Model
You may see examples from regular Transformer models where the input IDs are passed in as a positional argument (e.g., model.generate(input_ids)). For a PEFT model, they must be passed in as a keyword argument (e.g., model.generate(input_ids=input_ids)). For example:

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("gpt2")
inputs = tokenizer("Hello, my name is ", return_tensors="pt")
outputs = model.generate(input_ids=inputs["input_ids"], max_new_tokens=10)
print(tokenizer.batch_decode(outputs))